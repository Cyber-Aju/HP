{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYdHJ0NX9S2fBbjJdzQUQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cyber-Aju/HP/blob/main/HpSolve.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neo4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJCGsr72x_lX",
        "outputId": "11965ac7-d9da-4748-8bb6-b14254988b6f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.10/dist-packages (5.8.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2022.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g1jcghV0DSL",
        "outputId": "c310a76f-5f3a-4615-eef7-0a4dbc42d387"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "MXL2q5ifVCDJ",
        "outputId": "bf2d5267-0874-488d-fa64-7912f1d10c6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEDTYEwq9SkX",
        "outputId": "de93fb20-1c22-4639-d538-de80c827c652"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install linkedin-api\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNqzwJ73EGsf",
        "outputId": "7dc2523b-ed08-484b-d605-45e68a167452"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: linkedin-api in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from linkedin-api) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from linkedin-api) (4.11.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from linkedin-api) (4.9.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->linkedin-api) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->linkedin-api) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->linkedin-api) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->linkedin-api) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->linkedin-api) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set environment variables for LinkedIn API credentials\n",
        "os.environ['LINKEDIN_CLIENT_ID'] = '860sjyxfte5tlp'\n",
        "os.environ['LINKEDIN_CLIENT_SECRET'] = 'VEJKaFj9b2xMqIpF'\n",
        "os.environ['LINKEDIN_REDIRECT_URI'] = 'https://HpSolveByAjmal' # replace with your redirect URI"
      ],
      "metadata": {
        "id": "C9YdknQ7FrbE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "from linkedin_api import Linkedin\n",
        "\n",
        "# Authenticate with LinkedIn API using environment variables\n",
        "client = Linkedin(os.getenv('LINKEDIN_CLIENT_ID'), os.getenv('LINKEDIN_CLIENT_SECRET'))\n",
        "\n",
        "# Define search query for LinkedIn posts\n",
        "search_query = 'HP PC OR HP Printer OR HP laptop'\n",
        "\n",
        "# Collect posts containing the search query\n",
        "posts = client.search(search_query, 'post')\n",
        "\n",
        "# Iterate over each post and retrieve its comments\n",
        "for post in posts:\n",
        "    post_id = post['id']\n",
        "    print('Post ID:', post_id)\n",
        "    comments = client.get_comments(post_id)\n",
        "    print('Comments:', comments)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "XaamG2bHDqal",
        "outputId": "ed9f35fa-5d68-4d30-e6f5-3e9dbfaf36ac"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ChallengeException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mChallengeException\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-be16bcae8f44>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Authenticate with LinkedIn API using environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinkedin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LINKEDIN_CLIENT_ID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LINKEDIN_CLIENT_SECRET'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Define search query for LinkedIn posts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/linkedin_api/linkedin.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, username, password, authenticate, refresh_cookies, debug, proxies, cookies, cookies_dir)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_session_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevade\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_evade\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_request\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/linkedin_api/client.py\u001b[0m in \u001b[0;36mauthenticate\u001b[0;34m(self, username, password)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_authentication_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/linkedin_api/client.py\u001b[0m in \u001b[0;36m_do_authentication_request\u001b[0;34m(self, username, password)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"login_result\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"PASS\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mChallengeException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"login_result\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mChallengeException\u001b[0m: BAD_EMAIL"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install facebook-sdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRAATzHbJL6l",
        "outputId": "56981f52-ab5d-4a0d-c66f-a72a94683e44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting facebook-sdk\n",
            "  Downloading facebook_sdk-3.1.0-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from facebook-sdk) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->facebook-sdk) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->facebook-sdk) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->facebook-sdk) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->facebook-sdk) (3.4)\n",
            "Installing collected packages: facebook-sdk\n",
            "Successfully installed facebook-sdk-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "I2unxUX-xtLZ"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import spacy\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from neo4j import GraphDatabase"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Twitter API keys and tokens\n",
        "consumer_key = 'WlN1zRbamvD9U0cEqA3i0z3yC'\n",
        "consumer_secret = 'OKqt67JF3JDE2uSc6eOSUUv6yGVrbCFBgMu3E52b7yiZwKVSQa'\n",
        "access_token = '1457602236079542272-zBUlvZO6f2qCCvJsfC7QccnYEq2wcX'\n",
        "access_token_secret = '1XEM9dWZkhnzdBHmkmGuXtWd7Z1Sip2mFxVBvRUv2r6gz'\n",
        "\n",
        "# Authenticate with Twitter API using Tweepy library\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Define search query for HP products on Twitter\n",
        "search_query = 'HP PC OR HP Printer OR HP laptop OR HP DESKTOPS OR HP ALL-IN-ONE COMPUTER OR ENVY SERIES'\n",
        "\n",
        "'''\n",
        "# Collect tweets containing the search query in English language\n",
        "tweets = tweepy.Cursor(api.search_tweets,\n",
        "              q=search_query,\n",
        "              lang='en',\n",
        "              count=100).items()\n",
        " '''\n",
        "tweets = [\"Just got a new HP printer and I am loving it! Setup was easy and it prints beautifully #hpprinter #happycustomer\",\n",
        "          \"My HP laptop keeps crashing even though it's brand new. Anyone else experiencing this issue? #hplaptop #techsupport\",\n",
        "          \"Trying to decide between a Dell or HP PC for my home office. Any recommendations? #hppc #dellpc #decisionmaking\",\n",
        "          \"HP printers are so unreliable. Mine constantly jams and requires frequent maintenance. #hpprinter #unhappycustomer\",\n",
        "          \"Just upgraded to the new HP Spectre x360 and I am blown away by its performance and design. Highly recommend! #hplaptop #spectrex360 #upgrade\"\n",
        "          \"Dell printers are so unreliable. but hp are great #hp\"]\n"
      ],
      "metadata": {
        "id": "czVkTsRiyxJU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text data by removing links, mentions, hashtags, special characters, stop words, and stemming words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text) # remove links\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions\n",
        "    text = re.sub(r'#', '', text) # remove hashtags\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # remove special characters\n",
        "    text = text.lower() # convert to lowercase\n",
        "    words = word_tokenize(text)\n",
        "    stemmed_words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    text = ' '.join(stemmed_words) # stem words\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "7zvr0n6Bz8j6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates from tweets and preprocess them\n",
        "clean_tweets = list(set([preprocess_text(tweet_text) for tweet_text in tweets]))"
      ],
      "metadata": {
        "id": "KgFiK_-lUQq3"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "def get_sentiment(text):\n",
        "    scores = sia.polarity_scores(text)\n",
        "    if scores['compound'] > 0.05:\n",
        "        return 'positive'\n",
        "    elif scores['compound'] < -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "# Classify tweets as mentioning HP products or not using NaiveBayes classifier\n",
        "X = clean_tweets\n",
        "y = np.array([1 if \"hp\" in tweet.lower() else 0 for tweet in X])\n",
        "\n",
        "if len(np.unique(y)) < 2:\n",
        "    print(\"Error: y contains only one unique value.\")\n",
        "else:\n",
        "    pipeline = Pipeline([\n",
        "                          ('vectorizer', TfidfVectorizer()),\n",
        "                          ('clf', MultinomialNB())\n",
        "                        ])\n",
        "    parameters = {'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2)], \n",
        "                  'clf__alpha': np.linspace(0.5, 1.5, 6)}\n",
        "    grid_search = GridSearchCV(pipeline, parameters, cv=5)\n",
        "    grid_search.fit(X, y)\n",
        "\n",
        "    # Extract the best model from grid search\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "\n",
        "    # Classify tweets as mentioning HP products or not using the best model\n",
        "    predicted_classes = best_pipeline.predict(clean_tweets)\n",
        "    predicted_probabilities = best_pipeline.predict_proba(clean_tweets)[:, 1]\n",
        "\n",
        "    # Store labeled tweets in a dataframe\n",
        "    labeled_tweets = pd.DataFrame({\n",
        "        'text': clean_tweets,\n",
        "        'brand_model': predicted_classes, # 0 for non-HP, 1 for HP\n",
        "        'brand_model_probability': predicted_probabilities\n",
        "    })\n",
        "\n",
        "    # Load Spacy library for entity extraction\n",
        "    import spacy\n",
        "\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    def extract_entities(text):\n",
        "        doc = nlp(text)\n",
        "        entities = []\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in ['PRODUCT', 'ORG']:\n",
        "                entities.append(ent.text)\n",
        "        return entities\n",
        "\n",
        "    # Extract product models and features mentioned in tweets using Spacy library\n",
        "    labeled_tweets['entities'] = labeled_tweets['text'].apply(extract_entities)\n",
        "\n",
        "    # Analyze sentiment of tweets using Vader library\n",
        "    labeled_tweets['sentiment'] = labeled_tweets['text'].apply(get_sentiment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcMmb5ETyrNl",
        "outputId": "8188c90b-6d84-416f-c5ea-3c294a7d94dd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: y contains only one unique value.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ONLOwfd5yru3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0dtnvGSKyr38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set up connection to Neo4j instance\n",
        "uri = 'bolt://localhost:7687'\n",
        "username = 'Your Username Here'\n",
        "password = 'Your Password Here'\n",
        "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
        "\n",
        "# Define function for creating nodes and edges in the Neo4j knowledge graph\n",
        "def create_nodes(tx, entities):\n",
        "    for entity in entities:\n",
        "        tx.run(\"MERGE (n:\n",
        "\n",
        "\n",
        "Entity {name: $name})\", name=entity)\n",
        "\n",
        "def create_edges(tx, source_entity, target_entity, relationship_type):\n",
        "tx.run(\"\"\"\n",
        "MATCH (source:Entity {name: $source_entity})\n",
        "MATCH (target:Entity {name: $target_entity})\n",
        "MERGE (source)-[:%s]->(target)\n",
        "\"\"\" % relationship_type, source_entity=source_entity, target_entity=target_entity)\n",
        "Define function for querying the Neo4j knowledge graph based on a specific issue or topic\n",
        "\n",
        "def query_knowledge_graph(issue):\n",
        "with driver.session() as session:\n",
        "result = session.run(\"\"\"\n",
        "MATCH (e1:Entity)-[r]->(e2:Entity)\n",
        "WHERE e1.name = $issue OR e2.name = $issue\n",
        "RETURN e1.name AS source, type(r) AS relationship, e2.name AS target\n",
        "\"\"\", issue=issue)\n",
        "return result\n",
        "Run queries against the knowledge graph for each entity mentioned in the tweets\n",
        "\n",
        "for tweet in labeled_tweets.itertuples():\n",
        "entities = tweet.entities\n",
        "if tweet.brand_model == 1: # only create nodes and edges for HP products\n",
        "with driver.session() as session:\n",
        "session.write_transaction(create_nodes, entities)\n",
        "for i in range(len(entities)):\n",
        "for j in range(i+1, len(entities)):\n",
        "entity1 = entities[i]\n",
        "entity2 = entities[j]\n",
        "session.write_transaction(create_edges, entity1, entity2, tweet.sentiment)\n",
        "Print results of knowledge graph queries for each issue mentioned in the search query\n",
        "\n",
        "for issue in ['HP PC', 'HP Printer', 'HP laptop']:\n",
        "print('Results for {}:'.format(issue))\n",
        "result = query_knowledge_graph(issue)\n",
        "for record in result:\n",
        "print('{0} -{1}-> {2}'.format(record['source'], record['relationship'], record['target']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "Bdi4OSI9ysBk",
        "outputId": "36d3e2ee-b9dd-4375-9c20-843486058830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-c0d9688681f6>\"\u001b[0;36m, line \u001b[0;32m63\u001b[0m\n\u001b[0;31m    tx.run(\"MERGE (n:\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 63)\n"
          ]
        }
      ]
    }
  ]
}